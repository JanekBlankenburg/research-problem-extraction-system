{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Problem Extraction System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is to solve the problem as a Named Entity Recognition (NER) task, since the research problems to be extracted are components of the source texts. <br>\n",
    "For this purpose, a NER model will be trained first, for which the research questions from the training data will receive the entity 'RESEARCH_PROBLEM'. The sections of the test data for which this entity is recognized in the second step are predicted as research problems for the current paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pypickle as pypickle\n",
    "import json\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "\n",
    "LABEL = 'RESEARCH_PROBLEM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the system iterats through the directories over all papers and finally the extracted data are made available in a data structure. This consists of an array in which each paper has a dictionary, which contains the path to the folder, the title, the abstract and the labeled research questions for the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the training data\n",
    "training_data_path = \"training-data-master\"\n",
    "\n",
    "#Path to the test data\n",
    "test_data_path = \"test-data-master\"\n",
    "\n",
    "#Subpath to the json file containing the research questions\n",
    "research_questions_file_subpath = \"/info-units/research-problem.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "List all directories in a given directory\n",
    "\"\"\"\n",
    "def get_list_of_directories_in_directory(path:str):\n",
    "    return [ directory for directory in os.listdir(path) if os.path.isdir(os.path.join(path, directory))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "General data preprocessing\n",
    "\"\"\"\n",
    "def data_preprocessing(text:str):\n",
    "    #lower all texts\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the research-problem.json file for the current paper and extract the research problems\n",
    "\"\"\"\n",
    "def load_preprocess_problem(path:str):\n",
    "    with open(path + research_questions_file_subpath, 'r') as research_problem_file:\n",
    "        research_problem_file_data = json.load(research_problem_file)\n",
    "\n",
    "    if isinstance(research_problem_file_data[\"has research problem\"][0], list):\n",
    "        #more then one research problems\n",
    "        output_problems = [data_preprocessing(problem_array[0]) for problem_array in research_problem_file_data[\"has research problem\"]]\n",
    "    else:\n",
    "        #only one research problem\n",
    "        output_problems = research_problem_file_data[\"has research problem\"][0]\n",
    "        \n",
    "    return output_problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation about the structure of the paper of the provided output of the plaintext preprocessed by Stanza:<br>\n",
    "- First line:  \"titel\"<br>\n",
    "- Second line: [content of the title]<br>\n",
    "- Third line:  \"abstract\"<br>\n",
    "- Next lines:  [content of the title]<br>\n",
    "- Then:        \"introduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read the content of one paper and extrakt title and abstract\n",
    "\"\"\"\n",
    "def read_paper(path:str):\n",
    "    #read the content of the paper by reading the preprocessed file\n",
    "    content = data_preprocessing(open(path + \"/\" + [file for file in os.listdir(path) \n",
    "                                                         if re.match(\".*Stanza-out.txt\", file)][0], \"r\")\n",
    "                                                         .read()).split(\"\\n\")\n",
    "    #extract the title\n",
    "    title = content[1]\n",
    "    \n",
    "    #extract the abstract\n",
    "    abstract = \"\"\n",
    "    current_line=3\n",
    "    #the abstracts ends if the next line contains the word introduction\n",
    "    while \"introduction\" not in content[current_line] and current_line < 10:\n",
    "        abstract = abstract + \"\\n\" + content[current_line]\n",
    "        current_line += 1\n",
    "        \n",
    "    return title, abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find all paper in a sub-sub-directory of the given path, \n",
    "read the content and pass it into a dictionary-structure.\n",
    "\"\"\"\n",
    "def read_data(path:str):\n",
    "    paper_data = []\n",
    "    #iterate over all task folders \n",
    "    for task in get_list_of_directories_in_directory(training_data_path):\n",
    "        task_path = training_data_path + \"/\" + task\n",
    "        #iterate over all paper in the current task folder\n",
    "        for paper in get_list_of_directories_in_directory(training_data_path + \"/\" + task):\n",
    "            data = {}\n",
    "            #save the path of the current paper folder\n",
    "            data[\"path\"] = task_path + \"/\" + paper\n",
    "            #extract and save the title and the abstract of the current paper\n",
    "            data[\"title\"], data[\"abstract\"] = read_paper(data[\"path\"])\n",
    "            #extract and save the research problems of the current paper\n",
    "            data[\"research-problem\"]=load_preprocess_problem(data[\"path\"])\n",
    "            #add the current paper dictionary to an array containg all paper\n",
    "            paper_data.append(data)\n",
    "            \n",
    "    return paper_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paper_data = read_data(training_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test data set and split it in validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_test_paper_data = read_data(training_data_path)\n",
    "#split validation and test data in equal size\n",
    "validation_size = int(len(validation_test_paper_data) * 0.5)\n",
    "#randomly distribute test and validation data to prevent a bias with respect to the topics of the paper\n",
    "#using a seed to create reproducible results\n",
    "random.Random(0).shuffle(validation_test_paper_data)\n",
    "#build validation data\n",
    "validation_paper_data = validation_test_paper_data[validation_size:]\n",
    "#build test data\n",
    "test_paper_data = validation_test_paper_data[:validation_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'tagger', 'parser']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load spacy transformer model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "#disable unused pipeline elements\n",
    "nlp.disable_pipes('ner', 'tagger', 'parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build a dictionary used for training the NER model, containing spans with entities taged as \"RESEARCH_PROBLEM\"\n",
    "\"\"\"\n",
    "def create_ner_dictionary(paper_data:dict):\n",
    "    doc_bin = DocBin()\n",
    "    ner = {'classes' : ['RESEARCH_PROBLEM'], 'annotations': []}\n",
    "    for data in paper_data:\n",
    "        data_dic = {}\n",
    "        data_dic[\"text\"] = data[\"title\"]\n",
    "        #apply the language model\n",
    "        nlp_doc = nlp.make_doc(data_dic[\"text\"] )\n",
    "        data_dic['entities'] = []\n",
    "        ner_entities = []\n",
    "        #find spans of the research problems in the passed text\n",
    "        for problem in data[\"research-problem\"]:\n",
    "            if problem in data[\"title\"]:\n",
    "                #find start position\n",
    "                start_position = data[\"title\"].index(problem)\n",
    "                #find end position\n",
    "                end_position = start_position + len(problem)\n",
    "                #build span\n",
    "                data_dic['entities'].append((start_position, end_position, LABEL))\n",
    "                ner_span = nlp_doc.char_span(start_position, end_position, label=LABEL, alignment_mode=\"contract\")\n",
    "                if ner_span is not None:\n",
    "                    ner_entities.append(ner_span)\n",
    "            #remove duplicates or overlaps\n",
    "            nlp_doc.ents = filter_spans(ner_entities)\n",
    "            #add data to docBin\n",
    "            doc_bin.add(nlp_doc)\n",
    "        ner['annotations'].append(data_dic)\n",
    "        ner['doc_bin'] = doc_bin\n",
    "    return ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build NER dictionaries for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ner = create_ner_dictionary(train_paper_data)\n",
    "validation_ner = create_ner_dictionary(validation_paper_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save NER dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ner['doc_bin'].to_disk(\"training_data.spacy\")\n",
    "validation_ner['doc_bin'].to_disk(\"validation_data.spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilaiz the basic configuration for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\r\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\r\n",
      "config.cfg\r\n",
      "You can now add your data and train your pipeline:\r\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-07-06 15:47:19,212] [INFO] Set up nlp object from config\n",
      "[2022-07-06 15:47:19,220] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2022-07-06 15:47:19,223] [INFO] Created vocabulary\n",
      "[2022-07-06 15:47:19,224] [INFO] Finished initializing nlp object\n",
      "[2022-07-06 15:47:19,717] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     41.83    0.00    0.00    0.00    0.00\n",
      "  2     200         69.50   1986.98   95.03   93.18   96.96    0.95\n",
      "  6     400         93.40    262.92   97.59   99.30   95.95    0.98\n",
      " 10     600        139.88    183.77   96.86   94.82   98.99    0.97\n",
      " 15     800         84.67    146.23   97.18   95.44   98.99    0.97\n",
      " 21    1000        127.51    172.86   98.13   98.63   97.64    0.98\n",
      " 29    1200        138.82    181.70   97.18   95.44   98.99    0.97\n",
      " 39    1400        244.82    224.96   97.05   94.27  100.00    0.97\n",
      " 50    1600        364.51    282.26   98.28  100.00   96.62    0.98\n",
      " 64    1800        306.21    261.95   98.28  100.00   96.62    0.98\n",
      " 82    2000        111.58    325.76   98.13   98.63   97.64    0.98\n",
      "103    2200        116.23    369.76   98.13   98.63   97.64    0.98\n",
      "127    2400        179.52    425.72   98.28  100.00   96.62    0.98\n",
      "152    2600         88.86    388.23   98.28  100.00   96.62    0.98\n",
      "177    2800         62.82    379.24   98.28  100.00   96.62    0.98\n",
      "202    3000         31.59    367.79   98.13   98.63   97.64    0.98\n",
      "227    3200         34.17    369.03   98.28  100.00   96.62    0.98\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "output/model-last\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./output --paths.train ./training_data.spacy --paths.dev ./validation_data.spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the extracted information. (Only necessary for tests and reusage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pypickle] Pickle file saved: [train_paper_data]\n",
      "[pypickle] Pickle file saved: [validation_paper_data]\n",
      "[pypickle] Pickle file saved: [test_paper_data]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pypickle.save(\"train_paper_data\", train_paper_data)\n",
    "pypickle.save(\"validation_paper_data\", validation_paper_data)\n",
    "pypickle.save(\"test_paper_data\", test_paper_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the testdata. (Only necessary for tests and reusage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pypickle] Pickle file loaded: [test_paper_data]\n"
     ]
    }
   ],
   "source": [
    "test_paper_data = pypickle.load(\"test_paper_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric for testing the model:<br>\n",
    "- detected research-problem is part of training data: true positive<br>\n",
    "- detected research-problem is not part of training data: false positiv<br>\n",
    "- training data contains a research-problem that is not detected: false negativ<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 1.000000; recall: 0.331269; f1-score 0.497674\n"
     ]
    }
   ],
   "source": [
    "#count true positive\n",
    "tp = 0\n",
    "#count false positive\n",
    "fp = 0\n",
    "#count false negativ\n",
    "fn = 0\n",
    "\n",
    "for paper in test_paper_data:\n",
    "    #apply the learned model on the current testdata\n",
    "    doc = nlp_ner(paper[\"title\"])\n",
    "    detected_research_problems = [entity.text for entity in doc.ents]\n",
    "\n",
    "    for detected_research_problem in detected_research_problems:\n",
    "        if detected_research_problem in paper[\"research-problem\"]:\n",
    "            #correct predicted research problem\n",
    "            tp += 1\n",
    "        else:\n",
    "            #wrong predicted research problem\n",
    "            fp += 1\n",
    "    for labeled_research_problem in paper[\"research-problem\"]:\n",
    "        if labeled_research_problem not in detected_research_problems:\n",
    "            #labeled research problem was not predicted\n",
    "            fn += 1\n",
    "\n",
    "#calculate precision, recall and f1-score\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"precision: %f; recall: %f; f1-score %f\" % (precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
